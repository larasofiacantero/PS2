{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteo la ruta del repositorio (reemplazarla por la propia)\n",
    "ruta = r\"C:\\Users\\Fran\\MAESTRÍA\\Machine Learning\\Tarea 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijo rutas para cada carpeta del repositorio\n",
    "document_path = os.path.join(ruta, \"document\")\n",
    "scripts_path = os.path.join(ruta, \"scripts\")\n",
    "stores_path = os.path.join(ruta, \"stores\")\n",
    "views_path = os.path.join(ruta, \"views\")\n",
    "\n",
    "zip_path = os.path.join(stores_path, \"mlunlp-2024-ps-2.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hogares_path = \"train_hogares.csv\"\n",
    "train_personas_path = \"train_personas.csv\"\n",
    "test_hogares_path = \"test_hogares.csv\"\n",
    "test_personas_path = \"test_personas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_hogares_path) as f:\n",
    "        train_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_personas_path) as f:\n",
    "        train_personas = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_hogares_path) as f:\n",
    "        test_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_personas_path) as f:\n",
    "        test_personas = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.merge(train_personas, train_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  \n",
    "test_merged = pd.merge(test_personas, test_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación y transformación de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuestiones a tener en cuenta: \n",
    "\n",
    "Se cambió el formato de las variables \"P6210\", \"P6100\", \"P6585s3\", \"P6585s1\" de float a entero.\n",
    "Hacerlo también para las bases de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'Estrato1', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6500', 'P6510', 'P6510s1', 'P6510s2', 'P6545', 'P6545s1', 'P6545s2', 'P6580', 'P6580s1', 'P6580s2', 'P6585s1', 'P6585s1a1', 'P6585s1a2', 'P6585s2', 'P6585s2a1', 'P6585s2a2', 'P6585s3', 'P6585s3a1', 'P6585s3a2', 'P6585s4', 'P6585s4a1', 'P6585s4a2', 'P6590', 'P6590s1', 'P6600', 'P6600s1', 'P6610', 'P6610s1', 'P6620', 'P6620s1', 'P6630s1', 'P6630s1a1', 'P6630s2', 'P6630s2a1', 'P6630s3', 'P6630s3a1', 'P6630s4', 'P6630s4a1', 'P6630s6', 'P6630s6a1', 'P6750', 'P6760', 'P550', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7070', 'P7090', 'P7110', 'P7120', 'P7140s1', 'P7140s2', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7422s1', 'P7472', 'P7472s1', 'P7495', 'P7500s1', 'P7500s1a1', 'P7500s2', 'P7500s2a1', 'P7500s3', 'P7500s3a1', 'P7505', 'P7510s1', 'P7510s1a1', 'P7510s2', 'P7510s2a1', 'P7510s3', 'P7510s3a1', 'P7510s5', 'P7510s5a1', 'P7510s6', 'P7510s6a1', 'P7510s7', 'P7510s7a1', 'Pet', 'Oc', 'Des', 'Ina', 'Impa', 'Isa', 'Ie', 'Imdi', 'Iof1', 'Iof2', 'Iof3h', 'Iof3i', 'Iof6', 'Cclasnr2', 'Cclasnr3', 'Cclasnr4', 'Cclasnr5', 'Cclasnr6', 'Cclasnr7', 'Cclasnr8', 'Cclasnr11', 'Impaes', 'Isaes', 'Iees', 'Imdies', 'Iof1es', 'Iof2es', 'Iof3hes', 'Iof3ies', 'Iof6es', 'Ingtotob', 'Ingtotes', 'Ingtot', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Ingtotug', 'Ingtotugarr', 'Ingpcug', 'Li', 'Lp', 'Pobre', 'Indigente', 'Npobres', 'Nindigentes', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n",
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6510', 'P6545', 'P6580', 'P6585s1', 'P6585s2', 'P6585s3', 'P6585s4', 'P6590', 'P6600', 'P6610', 'P6620', 'P6630s1', 'P6630s2', 'P6630s3', 'P6630s4', 'P6630s6', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7090', 'P7110', 'P7120', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7472', 'P7495', 'P7500s2', 'P7500s3', 'P7505', 'P7510s1', 'P7510s2', 'P7510s3', 'P7510s5', 'P7510s6', 'P7510s7', 'Pet', 'Oc', 'Des', 'Ina', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Li', 'Lp', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n"
     ]
    }
   ],
   "source": [
    "columnas = train_merged.columns.tolist()\n",
    "print(columnas)\n",
    "\n",
    "columnas2 = test_merged.columns.tolist()\n",
    "print(columnas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la variable hacin\n",
    "train_merged[\"hacin\"] = train_merged[\"Nper\"] / train_merged[\"P5010\"]\n",
    "test_merged[\"hacin\"] = test_merged[\"Nper\"] / test_merged[\"P5010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 5 4 1 6 0 9]\n"
     ]
    }
   ],
   "source": [
    "# Transformo los valores de P6210\n",
    "train_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) \n",
    "print(train_merged[\"P6210\"].unique())\n",
    "\n",
    "test_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo el máximo nivel educativo por hogar\n",
    "educacion_max_hogar = train_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "educacion_max_hogar2 = test_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "\n",
    "# Renombro la columna\n",
    "educacion_max_hogar.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "educacion_max_hogar2.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "\n",
    "train_merged = train_merged.merge(educacion_max_hogar, on=\"id\", how=\"left\")\n",
    "test_merged = test_merged.merge(educacion_max_hogar2, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6100\n",
    "train_merged[\"P6100\"] = train_merged[\"P6100\"].fillna(0).astype(int) \n",
    "test_merged[\"P6100\"] = test_merged[\"P6100\"].fillna(0).astype(int) \n",
    "\n",
    "# Creo dummy con personas de régimen de salud subsidiado\n",
    "test_merged[\"salud_subsi\"] = (test_merged[\"P6100\"] == 3).astype(int)\n",
    "train_merged[\"salud_subsi\"] = (train_merged[\"P6100\"] == 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s3\n",
    "train_merged[\"P6585s3\"] = train_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s3\"] = test_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"fam_subsi\"] = (train_merged[\"P6585s3\"] == 1).astype(int)\n",
    "\n",
    "test_merged[\"fam_subsi\"] = (test_merged[\"P6585s3\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s1\n",
    "train_merged[\"P6585s1\"] = train_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s1\"] = test_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"alim_subsi\"] = (train_merged[\"P6585s1\"] == 1).astype(int)\n",
    "test_merged[\"alim_subsi\"] = (test_merged[\"P6585s1\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s2\n",
    "train_merged[\"P6585s2\"] = train_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s2\"] = test_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"transp_subsi\"] = (train_merged[\"P6585s2\"] == 1).astype(int)\n",
    "test_merged[\"transp_subsi\"] = (test_merged[\"P6585s2\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el primer approach\n",
    "train_pobreza = train_merged[[\"id\", \"Orden\", \"P6040\", \"hacin\", \"Clase_hogar\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"alim_subsi\", \"transp_subsi\", \"P6430\", \"Lp\", \"Pobre\"]]\n",
    "test_pobreza = test_merged[[\"id\", \"Orden\", \"P6040\", \"hacin\", \"Clase_hogar\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"alim_subsi\", \"transp_subsi\", \"P6430\", \"Lp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el segundo approach\n",
    "\n",
    "train_ingreso = train_merged[[\"id\", \"Orden\", \"Ingtot\", \"P6020\", \"P6040\", \"P6020\", \"P6920\", \"P6800\", \"P6210\", \"P6426\", \"P6430\",\"educ_max\", \"Lp\"]]\n",
    "test_ingreso = test_merged[[\"id\", \"Orden\", \"P6020\", \"P6040\", \"P6020\", \"P6920\", \"P6800\", \"P6210\", \"P6426\", \"P6430\",\"educ_max\", \"Lp\"]]\n",
    "\n",
    "# \"P6040\": edad, \n",
    "# \"P6020\": sexo, \n",
    "# \"P6920\": cotiza pension, \n",
    "# \"P6800\": horas de trabajo semanal normal, \n",
    "# \"P6210\": máximo nivel educativo, \n",
    "# \"P6426\": antigüedad, \n",
    "# \"P6920\": tipo de ocupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression\n",
      "Exactitud en entrenamiento: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90    116213\n",
      "           1       0.59      0.26      0.36     26524\n",
      "\n",
      "    accuracy                           0.83    142737\n",
      "   macro avg       0.72      0.61      0.63    142737\n",
      "weighted avg       0.80      0.83      0.80    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net\n",
      "Exactitud en entrenamiento: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90    116213\n",
      "           1       0.77      0.03      0.05     26524\n",
      "\n",
      "    accuracy                           0.82    142737\n",
      "   macro avg       0.79      0.51      0.47    142737\n",
      "weighted avg       0.81      0.82      0.74    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree\n",
      "Exactitud en entrenamiento: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    116213\n",
      "           1       1.00      0.96      0.98     26524\n",
      "\n",
      "    accuracy                           0.99    142737\n",
      "   macro avg       0.99      0.98      0.99    142737\n",
      "weighted avg       0.99      0.99      0.99    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree (max_depth=10)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.68      0.40      0.50     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.68      0.71    142737\n",
      "weighted avg       0.84      0.85      0.84    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest\n",
      "Exactitud en entrenamiento: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    116213\n",
      "           1       0.99      0.97      0.98     26524\n",
      "\n",
      "    accuracy                           0.99    142737\n",
      "   macro avg       0.99      0.99      0.99    142737\n",
      "weighted avg       0.99      0.99      0.99    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest (n_estimators=200, max_depth=10)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91    116213\n",
      "           1       0.71      0.33      0.45     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.79      0.65      0.68    142737\n",
      "weighted avg       0.84      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.67      0.36      0.47     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.66      0.69    142737\n",
      "weighted avg       0.83      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (n_estimators=200, learning_rate=0.05)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.67      0.36      0.47     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.66      0.69    142737\n",
      "weighted avg       0.83      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Resultados finales:\n",
      "Logistic Regression: 0.83\n",
      "Elastic Net: 0.82\n",
      "Decision Tree: 0.99\n",
      "Decision Tree (max_depth=10): 0.85\n",
      "Random Forest: 0.99\n",
      "Random Forest (n_estimators=200, max_depth=10): 0.85\n",
      "Gradient Boosting: 0.85\n",
      "Gradient Boosting (n_estimators=200, learning_rate=0.05): 0.85\n",
      "Proporción de hogares pobres predicha: 19.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "# Selección de variables relevantes y eliminación de valores faltantes\n",
    "data_train = train_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\", \"Pobre\"]].dropna()\n",
    "data_test = test_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\"]].dropna()\n",
    "\n",
    "# Crear un dataset a nivel de hogar agrupando por id\n",
    "hogares_train = data_train.groupby('id').mean()\n",
    "hogares_train['Pobre'] = data_train.groupby('id')['Pobre'].max()  # Si algún miembro es pobre, se clasifica el hogar como pobre\n",
    "hogares_test = data_test.groupby('id').mean()\n",
    "\n",
    "# Definir variables predictoras y objetivo para entrenamiento\n",
    "X_train = hogares_train.drop(columns=['Pobre'])\n",
    "y_train = hogares_train['Pobre']\n",
    "\n",
    "# Definir variables predictoras para prueba\n",
    "X_test = hogares_test\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Decision Tree (max_depth=10)': DecisionTreeClassifier(max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest (n_estimators=200, max_depth=10)': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'Gradient Boosting (n_estimators=200, learning_rate=0.05)': GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_train)  # Evaluación en datos de entrenamiento\n",
    "    if nombre == 'Elastic Net':\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Convertir predicciones en binarias para Elastic Net\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    resultados[nombre] = accuracy\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"Exactitud en entrenamiento: {accuracy:.2f}\")\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"Resultados finales:\")\n",
    "for modelo, accuracy in resultados.items():\n",
    "    print(f\"{modelo}: {accuracy:.2f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_model_name = max(resultados, key=resultados.get)\n",
    "best_model = modelos[best_model_name]\n",
    "\n",
    "# Crear pipeline final\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Entrenar pipeline final\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones finales\n",
    "hogares_test['Predicted_Pobre'] = final_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular proporción de hogares pobres en el conjunto de prueba\n",
    "prop_pobres = hogares_test['Predicted_Pobre'].mean()\n",
    "print(f\"Proporción de hogares pobres predicha: {prop_pobres:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': 0.8291262952142752,\n",
       " 'Elastic Net': 0.8175665734883036,\n",
       " 'Decision Tree': 0.9928189607459874,\n",
       " 'Decision Tree (max_depth=10)': 0.8526310627237507,\n",
       " 'Random Forest': 0.9927629136103463,\n",
       " 'Random Forest (n_estimators=200, max_depth=10)': 0.8506904306521785,\n",
       " 'Gradient Boosting': 0.8475798146240988,\n",
       " 'Gradient Boosting (n_estimators=200, learning_rate=0.05)': 0.847691908895381}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
