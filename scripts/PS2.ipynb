{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteo la ruta del repositorio (reemplazarla por la propia)\n",
    "ruta = r\"C:\\Users\\Fran\\MAESTRÍA\\Machine Learning\\Tarea 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijo rutas para cada carpeta del repositorio\n",
    "document_path = os.path.join(ruta, \"document\")\n",
    "scripts_path = os.path.join(ruta, \"scripts\")\n",
    "stores_path = os.path.join(ruta, \"stores\")\n",
    "views_path = os.path.join(ruta, \"views\")\n",
    "\n",
    "zip_path = os.path.join(stores_path, \"mlunlp-2024-ps-2.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hogares_path = \"train_hogares.csv\"\n",
    "train_personas_path = \"train_personas.csv\"\n",
    "test_hogares_path = \"test_hogares.csv\"\n",
    "test_personas_path = \"test_personas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_hogares_path) as f:\n",
    "        train_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_personas_path) as f:\n",
    "        train_personas = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_hogares_path) as f:\n",
    "        test_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_personas_path) as f:\n",
    "        test_personas = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.merge(train_personas, train_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  \n",
    "test_merged = pd.merge(test_personas, test_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación y transformación de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuestiones a tener en cuenta: \n",
    "\n",
    "Se cambió el formato de las variables \"P6210\", \"P6100\", \"P6585s3\", \"P6585s1\" de float a entero.\n",
    "Hacerlo también para las bases de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'Estrato1', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6500', 'P6510', 'P6510s1', 'P6510s2', 'P6545', 'P6545s1', 'P6545s2', 'P6580', 'P6580s1', 'P6580s2', 'P6585s1', 'P6585s1a1', 'P6585s1a2', 'P6585s2', 'P6585s2a1', 'P6585s2a2', 'P6585s3', 'P6585s3a1', 'P6585s3a2', 'P6585s4', 'P6585s4a1', 'P6585s4a2', 'P6590', 'P6590s1', 'P6600', 'P6600s1', 'P6610', 'P6610s1', 'P6620', 'P6620s1', 'P6630s1', 'P6630s1a1', 'P6630s2', 'P6630s2a1', 'P6630s3', 'P6630s3a1', 'P6630s4', 'P6630s4a1', 'P6630s6', 'P6630s6a1', 'P6750', 'P6760', 'P550', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7070', 'P7090', 'P7110', 'P7120', 'P7140s1', 'P7140s2', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7422s1', 'P7472', 'P7472s1', 'P7495', 'P7500s1', 'P7500s1a1', 'P7500s2', 'P7500s2a1', 'P7500s3', 'P7500s3a1', 'P7505', 'P7510s1', 'P7510s1a1', 'P7510s2', 'P7510s2a1', 'P7510s3', 'P7510s3a1', 'P7510s5', 'P7510s5a1', 'P7510s6', 'P7510s6a1', 'P7510s7', 'P7510s7a1', 'Pet', 'Oc', 'Des', 'Ina', 'Impa', 'Isa', 'Ie', 'Imdi', 'Iof1', 'Iof2', 'Iof3h', 'Iof3i', 'Iof6', 'Cclasnr2', 'Cclasnr3', 'Cclasnr4', 'Cclasnr5', 'Cclasnr6', 'Cclasnr7', 'Cclasnr8', 'Cclasnr11', 'Impaes', 'Isaes', 'Iees', 'Imdies', 'Iof1es', 'Iof2es', 'Iof3hes', 'Iof3ies', 'Iof6es', 'Ingtotob', 'Ingtotes', 'Ingtot', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Ingtotug', 'Ingtotugarr', 'Ingpcug', 'Li', 'Lp', 'Pobre', 'Indigente', 'Npobres', 'Nindigentes', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n",
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6510', 'P6545', 'P6580', 'P6585s1', 'P6585s2', 'P6585s3', 'P6585s4', 'P6590', 'P6600', 'P6610', 'P6620', 'P6630s1', 'P6630s2', 'P6630s3', 'P6630s4', 'P6630s6', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7090', 'P7110', 'P7120', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7472', 'P7495', 'P7500s2', 'P7500s3', 'P7505', 'P7510s1', 'P7510s2', 'P7510s3', 'P7510s5', 'P7510s6', 'P7510s7', 'Pet', 'Oc', 'Des', 'Ina', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Li', 'Lp', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n"
     ]
    }
   ],
   "source": [
    "columnas = train_merged.columns.tolist()\n",
    "print(columnas)\n",
    "\n",
    "columnas2 = test_merged.columns.tolist()\n",
    "print(columnas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la variable hacin\n",
    "train_merged[\"hacin\"] = train_merged[\"Nper\"] / train_merged[\"P5010\"]\n",
    "test_merged[\"hacin\"] = test_merged[\"Nper\"] / test_merged[\"P5010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 5 4 1 6 0 9]\n"
     ]
    }
   ],
   "source": [
    "# Transformo los valores de P6210\n",
    "train_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) \n",
    "print(train_merged[\"P6210\"].unique())\n",
    "\n",
    "test_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo el máximo nivel educativo por hogar\n",
    "educacion_max_hogar = train_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "educacion_max_hogar2 = test_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "\n",
    "# Renombro la columna\n",
    "educacion_max_hogar.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "educacion_max_hogar2.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "\n",
    "train_merged = train_merged.merge(educacion_max_hogar, on=\"id\", how=\"left\")\n",
    "test_merged = test_merged.merge(educacion_max_hogar2, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6100\n",
    "train_merged[\"P6100\"] = train_merged[\"P6100\"].fillna(0).astype(int) \n",
    "test_merged[\"P6100\"] = test_merged[\"P6100\"].fillna(0).astype(int) \n",
    "\n",
    "# Creo dummy con personas de régimen de salud subsidiado\n",
    "test_merged[\"salud_subsi\"] = (test_merged[\"P6100\"] == 3).astype(int)\n",
    "train_merged[\"salud_subsi\"] = (train_merged[\"P6100\"] == 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s3\n",
    "train_merged[\"P6585s3\"] = train_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s3\"] = test_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"fam_subsi\"] = (train_merged[\"P6585s3\"] == 1).astype(int)\n",
    "\n",
    "test_merged[\"fam_subsi\"] = (test_merged[\"P6585s3\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s1\n",
    "train_merged[\"P6585s1\"] = train_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s1\"] = test_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"alim_subsi\"] = (train_merged[\"P6585s1\"] == 1).astype(int)\n",
    "test_merged[\"alim_subsi\"] = (test_merged[\"P6585s1\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s2\n",
    "train_merged[\"P6585s2\"] = train_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s2\"] = test_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"transp_subsi\"] = (train_merged[\"P6585s2\"] == 1).astype(int)\n",
    "test_merged[\"transp_subsi\"] = (test_merged[\"P6585s2\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el primer approach\n",
    "train_pobreza = train_merged[[\"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\", \"Pobre\"]]\n",
    "test_pobreza = test_merged[[\"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el segundo approach\n",
    "\n",
    "train_ingreso = train_merged[[ \"P6020\", \"hacin\", \"P6040\", \"P6920\", \"P6800\", \"P6210\", \"P6240\", \"educ_max\",\"Ingtot\", \"P6426\", \"fam_subsi\", \"P6430\", \"P7050\", \"Lp\"]]\n",
    "test_ingreso = test_merged[[ \"P6020\", \"hacin\", \"P6040\", \"P6920\", \"P6800\", \"P6210\", \"P6240\", \"educ_max\", \"P6426\", \"P6430\", \"fam_subsi\", \"P7050\", \"Lp\"]]\n",
    "\n",
    "\n",
    "# \"P6040\": edad, \n",
    "# \"P6020\": sexo, \n",
    "# \"P6920\": cotiza pension, \n",
    "# \"P6800\": horas de trabajo semanal normal, \n",
    "# \"P6210\": máximo nivel educativo, \n",
    "# \"P6426\": antigüedad, \n",
    "# \"P6920\": tipo de ocupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputación completada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\1379995776.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Función para imputar valores NaN usando regresión\n",
    "def impute_with_regression(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            # Separar datos completos e incompletos\n",
    "            train_complete = df[df[col].notnull()]\n",
    "            train_missing = df[df[col].isnull()]\n",
    "            \n",
    "            # Seleccionar predictores no nulos\n",
    "            predictors = train_complete.drop(columns=[col]).dropna(axis=1)\n",
    "            if predictors.empty:  # Si no hay predictores disponibles\n",
    "                continue\n",
    "            \n",
    "            # Imputar valores faltantes en los predictores de train_missing\n",
    "            missing_predictors = train_missing[predictors.columns]\n",
    "            missing_predictors.fillna(predictors.mean(), inplace=True)  # Imputar media en los predictores\n",
    "            \n",
    "            # Entrenar el modelo predictivo\n",
    "            model = LinearRegression()\n",
    "            model.fit(predictors, train_complete[col])\n",
    "            \n",
    "            # Predecir los valores faltantes\n",
    "            df.loc[df[col].isnull(), col] = model.predict(missing_predictors)\n",
    "    return df\n",
    "\n",
    "# Convertir las bases a DataFrames si no lo son\n",
    "train_pobreza = pd.DataFrame(train_pobreza)\n",
    "test_pobreza = pd.DataFrame(test_pobreza)\n",
    "train_ingreso = pd.DataFrame(train_ingreso)\n",
    "test_ingreso = pd.DataFrame(test_ingreso)\n",
    "\n",
    "# Aplicar la imputación a cada base\n",
    "train_pobreza = impute_with_regression(train_pobreza)\n",
    "test_pobreza = impute_with_regression(test_pobreza)\n",
    "train_ingreso = impute_with_regression(train_ingreso)\n",
    "test_ingreso = impute_with_regression(test_ingreso)\n",
    "\n",
    "# Mostrar resultados (opcional)\n",
    "print(\"Imputación completada.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar la variable \"id\" de train_merged\n",
    "id_column = train_merged[[\"id\"]]\n",
    "id_column2 = test_merged[[\"id\"]]\n",
    "\n",
    "# Combinar las bases\n",
    "train_ingreso = pd.concat([train_ingreso, id_column], axis=1)\n",
    "test_ingreso = pd.concat([test_ingreso, id_column2], axis=1)\n",
    "train_pobreza = pd.concat([train_pobreza, id_column], axis=1)\n",
    "test_pobreza = pd.concat([test_pobreza, id_column2], axis=1)\n",
    "\n",
    "train_ingreso = train_ingreso.loc[:, ~train_ingreso.columns.duplicated()]\n",
    "test_ingreso = test_ingreso.loc[:, ~test_ingreso.columns.duplicated()]\n",
    "train_pobreza = train_pobreza.loc[:, ~train_pobreza.columns.duplicated()]\n",
    "test_pobreza = test_pobreza.loc[:, ~test_pobreza.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression\n",
      "Exactitud en entrenamiento: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90    131936\n",
      "           1       0.61      0.33      0.43     33024\n",
      "\n",
      "    accuracy                           0.82    164960\n",
      "   macro avg       0.73      0.64      0.66    164960\n",
      "weighted avg       0.80      0.82      0.80    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net\n",
      "Exactitud en entrenamiento: 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89    131936\n",
      "           1       0.60      0.02      0.04     33024\n",
      "\n",
      "    accuracy                           0.80    164960\n",
      "   macro avg       0.70      0.51      0.46    164960\n",
      "weighted avg       0.76      0.80      0.72    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree\n",
      "Exactitud en entrenamiento: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    131936\n",
      "           1       1.00      1.00      1.00     33024\n",
      "\n",
      "    accuracy                           1.00    164960\n",
      "   macro avg       1.00      1.00      1.00    164960\n",
      "weighted avg       1.00      1.00      1.00    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree (max_depth=10)\n",
      "Exactitud en entrenamiento: 0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92    131936\n",
      "           1       0.71      0.56      0.62     33024\n",
      "\n",
      "    accuracy                           0.87    164960\n",
      "   macro avg       0.80      0.75      0.77    164960\n",
      "weighted avg       0.86      0.87      0.86    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest\n",
      "Exactitud en entrenamiento: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    131936\n",
      "           1       1.00      1.00      1.00     33024\n",
      "\n",
      "    accuracy                           1.00    164960\n",
      "   macro avg       1.00      1.00      1.00    164960\n",
      "weighted avg       1.00      1.00      1.00    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest (n_estimators=200, max_depth=10)\n",
      "Exactitud en entrenamiento: 0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92    131936\n",
      "           1       0.76      0.49      0.60     33024\n",
      "\n",
      "    accuracy                           0.87    164960\n",
      "   macro avg       0.82      0.73      0.76    164960\n",
      "weighted avg       0.86      0.87      0.86    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting\n",
      "Exactitud en entrenamiento: 0.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91    131936\n",
      "           1       0.71      0.48      0.57     33024\n",
      "\n",
      "    accuracy                           0.86    164960\n",
      "   macro avg       0.80      0.72      0.74    164960\n",
      "weighted avg       0.85      0.86      0.85    164960\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (n_estimators=200, learning_rate=0.05)\n",
      "Exactitud en entrenamiento: 0.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91    131936\n",
      "           1       0.71      0.47      0.57     33024\n",
      "\n",
      "    accuracy                           0.86    164960\n",
      "   macro avg       0.80      0.71      0.74    164960\n",
      "weighted avg       0.85      0.86      0.85    164960\n",
      "\n",
      "----------------------------------------\n",
      "Resultados finales:\n",
      "Logistic Regression: 0.82\n",
      "Elastic Net: 0.80\n",
      "Decision Tree: 1.00\n",
      "Decision Tree (max_depth=10): 0.87\n",
      "Random Forest: 1.00\n",
      "Random Forest (n_estimators=200, max_depth=10): 0.87\n",
      "Gradient Boosting: 0.86\n",
      "Gradient Boosting (n_estimators=200, learning_rate=0.05): 0.86\n",
      "Proporción de hogares pobres predicha: 19.07%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "# Selección de variables relevantes y eliminación de valores faltantes\n",
    "data_train = train_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"P6430\", \"Lp\", \"Pobre\"]]\n",
    "data_test = test_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"P6430\", \"Lp\"]]\n",
    "\n",
    "# Crear un dataset a nivel de hogar agrupando por id\n",
    "hogares_train = data_train.groupby('id').mean()\n",
    "hogares_train['Pobre'] = data_train.groupby('id')['Pobre'].max()  # Si algún miembro es pobre, se clasifica el hogar como pobre\n",
    "hogares_test = data_test.groupby('id').mean()\n",
    "\n",
    "# Definir variables predictoras y objetivo para entrenamiento\n",
    "X_train = hogares_train.drop(columns=['Pobre'])\n",
    "y_train = hogares_train['Pobre']\n",
    "\n",
    "# Definir variables predictoras para prueba\n",
    "X_test = hogares_test\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Decision Tree (max_depth=10)': DecisionTreeClassifier(max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest (n_estimators=200, max_depth=10)': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'Gradient Boosting (n_estimators=200, learning_rate=0.05)': GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_train)  # Evaluación en datos de entrenamiento\n",
    "    if nombre == 'Elastic Net':\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Convertir predicciones en binarias para Elastic Net\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    resultados[nombre] = accuracy\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"Exactitud en entrenamiento: {accuracy:.2f}\")\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"Resultados finales:\")\n",
    "for modelo, accuracy in resultados.items():\n",
    "    print(f\"{modelo}: {accuracy:.2f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_model_name = max(resultados, key=resultados.get)\n",
    "best_model = modelos[best_model_name]\n",
    "\n",
    "# Crear pipeline final\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Entrenar pipeline final\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones finales\n",
    "hogares_test['Predicted_Pobre'] = final_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular proporción de hogares pobres en el conjunto de prueba\n",
    "prop_pobres = hogares_test['Predicted_Pobre'].mean()\n",
    "print(f\"Proporción de hogares pobres predicha: {prop_pobres:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': 0.8228358389912707,\n",
       " 'Elastic Net': 0.8010972356935014,\n",
       " 'Decision Tree': 0.9993937924345296,\n",
       " 'Decision Tree (max_depth=10)': 0.8652703685741998,\n",
       " 'Random Forest': 0.9993877303588748,\n",
       " 'Random Forest (n_estimators=200, max_depth=10)': 0.8679801163918526,\n",
       " 'Gradient Boosting': 0.8574745392822503,\n",
       " 'Gradient Boosting (n_estimators=200, learning_rate=0.05)': 0.8568804558680893}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogares_test.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el nuevo dataset con las columnas seleccionadas\n",
    "resultados_pobreza = hogares_test[['id', 'Predicted_Pobre']].copy()\n",
    "\n",
    "# Renombrar la columna 'Predicted_Pobre' a 'pobre'\n",
    "resultados_pobreza.rename(columns={'Predicted_Pobre': 'pobre'}, inplace=True)\n",
    "\n",
    "resultados_pobreza = resultados_pobreza[['id', 'pobre']]\n",
    "\n",
    "# Exportar el DataFrame a un archivo CSV sin índice\n",
    "resultados_pobreza.to_csv(ruta + '/classification_decisiontree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de los datos\n",
    "data_train = train_ingreso[[\"id\", \"P6040\", \"educ_max\", \"P6800\", \"P6426\", \"P7050\", \"P6430\", \"Lp\", \"Ingtot\"]]\n",
    "data_test = test_ingreso[[\"id\", \"P6040\", \"educ_max\", \"P6800\", \"P6426\", \"P7050\", \"P6430\", \"Lp\"]]\n",
    "\n",
    "\n",
    "# Genero la linea de pobreza de cada hogar\n",
    "test_ingreso['lp_total'] = test_ingreso.groupby('id')['Lp'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n",
    "# En un momento se duplicaron algunas columnas, corremos lo siguiente para corregirlo si hiciera falta\n",
    "data_train = data_train.loc[:, ~data_train.columns.duplicated()]\n",
    "data_test = data_test.loc[:, ~data_test.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Linear Regression\n",
      "MSE en entrenamiento: 1225090547409.19\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net\n",
      "MSE en entrenamiento: 1226613536149.51\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net (alpha=1)\n",
      "MSE en entrenamiento: 1254275893161.64\n",
      "----------------------------------------\n",
      "Modelo: Random Forest\n",
      "MSE en entrenamiento: 358826076178.07\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree (CART)\n",
      "MSE en entrenamiento: 98861.48\n",
      "----------------------------------------\n",
      "Modelo: AdaBoost Regressor\n",
      "MSE en entrenamiento: 7259407886295.95\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (learning_rate=0.1)\n",
      "MSE en entrenamiento: 460560372596.55\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (learning_rate=0.05)\n",
      "MSE en entrenamiento: 553598644279.69\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asegurarse de que Ingtot esté en formato numérico\n",
    "data_train['Ingtot'] = pd.to_numeric(data_train['Ingtot'], errors='coerce')\n",
    "\n",
    "# Eliminar filas con valores faltantes en el conjunto de entrenamiento\n",
    "data_train = data_train.dropna()\n",
    "\n",
    "# Nombre de la columna que contiene el ID\n",
    "id_column = 'id'  # Cambia esto si el nombre de la columna ID es diferente\n",
    "\n",
    "# Excluir 'id' al definir las variables predictoras\n",
    "X = data_train.drop(columns=['Ingtot', id_column])\n",
    "y = data_train['Ingtot']\n",
    "\n",
    "X_train = X\n",
    "y_train = y\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Elastic Net (alpha=1)': ElasticNet(alpha=1, l1_ratio=0.7),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42),\n",
    "    'Decision Tree (CART)': DecisionTreeRegressor(random_state=42),\n",
    "    'AdaBoost Regressor': AdaBoostRegressor(random_state=42),\n",
    "    'Gradient Boosting (learning_rate=0.1)': GradientBoostingRegressor(learning_rate=0.1, random_state=42),\n",
    "    'Gradient Boosting (learning_rate=0.05)': GradientBoostingRegressor(learning_rate=0.05, random_state=42),\n",
    "}\n",
    "\n",
    "# Entrenamiento y predicciones\n",
    "resultados = {}\n",
    "mejor_modelo = None\n",
    "mejor_mse = float('inf')\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    # Crear pipeline (escalado + modelo)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Escalado de variables\n",
    "        ('model', modelo)\n",
    "    ])\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones en el conjunto de entrenamiento (para evaluación)\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    resultados[nombre] = mse_train\n",
    "    \n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"MSE en entrenamiento: {mse_train:.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Verificar si es el mejor modelo\n",
    "    if mse_train < mejor_mse:\n",
    "        mejor_mse = mse_train\n",
    "        mejor_modelo = pipeline\n",
    "\n",
    "# Asegurarse de que las columnas coincidan entre X_train y X_test\n",
    "X_test = data_test[X_train.columns]\n",
    "\n",
    "# Predicciones finales en el conjunto de prueba\n",
    "data_test['Ingtot_predicho'] = mejor_modelo.predict(X_test)\n",
    "\n",
    "# Asegurar que los valores negativos sean cero\n",
    "data_test['Ingtot_predicho'] = np.maximum(data_test['Ingtot_predicho'], 0)\n",
    "\n",
    "# Crear un DataFrame con los resultados finales (ID + Predicción del mejor modelo)\n",
    "resultados_finales = data_test[[id_column, 'Ingtot_predicho']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model', DecisionTreeRegressor(random_state=42))]) con MSE: 98861.48\n",
      "                              id  Ingtot_predicho\n",
      "0       3279230a4917cdf883df34cd     8.000000e+05\n",
      "1       3279230a4917cdf883df34cd     9.391603e+05\n",
      "2       3279230a4917cdf883df34cd     3.300000e+05\n",
      "3       3279230a4917cdf883df34cd     9.391603e+05\n",
      "4       01bd1f72445acc719d19bd25     1.000000e+06\n",
      "...                          ...              ...\n",
      "219639  95aca5f4d202450cdfc16c37     2.000000e+06\n",
      "219640  95aca5f4d202450cdfc16c37     6.900000e+05\n",
      "219641  95aca5f4d202450cdfc16c37     5.000000e+05\n",
      "219642  95aca5f4d202450cdfc16c37     3.000000e+05\n",
      "219643  95aca5f4d202450cdfc16c37     0.000000e+00\n",
      "\n",
      "[219644 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el mejor modelo y los resultados\n",
    "print(f\"\\nMejor modelo: {mejor_modelo} con MSE: {mejor_mse:.2f}\")\n",
    "print(resultados_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\2379261951.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['lp_total'] = test_ingreso['lp_total']\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\2379261951.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['ingtot_total'] = resultados_finales.groupby('id')['Ingtot_predicho'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de hogares por debajo de la línea de pobreza: 18.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\2379261951.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['por_bajo_lp'] = resultados_finales.apply(lambda row: 1 if row['ingtot_total'] < row['lp_total'] else 0, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar que proporcion de los hogares son pobres\n",
    "resultados_finales['lp_total'] = test_ingreso['lp_total']\n",
    "resultados_finales['ingtot_total'] = resultados_finales.groupby('id')['Ingtot_predicho'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n",
    "resultados_finales['por_bajo_lp'] = resultados_finales.apply(lambda row: 1 if row['ingtot_total'] < row['lp_total'] else 0, axis=1)\n",
    "porcentaje_bajo_lp = resultados_finales['por_bajo_lp'].mean() * 100\n",
    "print(f\"Proporción de hogares por debajo de la línea de pobreza: {porcentaje_bajo_lp:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_23184\\716926711.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales.drop(columns=['Ingtot_predicho'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             id      lp_total  ingtot_total  por_bajo_lp\n",
      "0      00007322a7918c7799e7a514  2.736141e+05  8.000000e+05            0\n",
      "1      0004847804c0618d4633b196  1.161989e+06  4.366667e+06            0\n",
      "2      0004add76858a3b0cc39eb9b  6.885295e+06  2.848750e+07            0\n",
      "3      000804ac64d779b35ee5edfa  9.790772e+06  5.260000e+06            1\n",
      "4      000b6a116b7f95b043a6e77b  6.706878e+05  2.736422e+06            0\n",
      "...                         ...           ...           ...          ...\n",
      "66163  fff937c57942f839f144183f  6.131130e+06  1.984000e+07            0\n",
      "66164  fffb951f761dfc6ee518a862  1.337400e+07  4.151862e+07            0\n",
      "66165  fffb97493bad434012c34ff2  4.572971e+06  2.277225e+06            1\n",
      "66166  fffe1f02fa2758a80522d514  1.089979e+07  8.900312e+06            1\n",
      "66167  ffffd05852760b7f8823f298  4.523979e+06  1.372117e+07            0\n",
      "\n",
      "[66168 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "resultados_finales.drop(columns=['Ingtot_predicho'], inplace=True)\n",
    "\n",
    "# Agrupar por 'id' y calcular la suma de las otras columnas\n",
    "resultados_agrupados = resultados_finales.groupby('id').sum().reset_index()\n",
    "\n",
    "resultados_agrupados['por_bajo_lp'] = resultados_agrupados['por_bajo_lp'].apply(lambda x: 1 if x > 0 else x)\n",
    "\n",
    "# Mostrar el resultado agrupado\n",
    "print(resultados_agrupados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el nuevo dataset con las columnas seleccionadas\n",
    "resultados_ingresos_pobreza = resultados_agrupados[['id', 'por_bajo_lp']].copy()\n",
    "\n",
    "# Renombrar la columna 'por_bajo_lp' a 'pobre'\n",
    "resultados_ingresos_pobreza.rename(columns={'por_bajo_lp': 'pobre'}, inplace=True)\n",
    "\n",
    "resultados_ingresos_pobreza = resultados_ingresos_pobreza[['id', 'pobre']]\n",
    "\n",
    "# Exportar el DataFrame a un archivo CSV sin índice\n",
    "resultados_ingresos_pobreza.to_csv(ruta + '/regression_decisiontree.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
