{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteo la ruta del repositorio (reemplazarla por la propia)\n",
    "ruta = r\"C:\\Users\\Fran\\MAESTRÍA\\Machine Learning\\Tarea 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijo rutas para cada carpeta del repositorio\n",
    "document_path = os.path.join(ruta, \"document\")\n",
    "scripts_path = os.path.join(ruta, \"scripts\")\n",
    "stores_path = os.path.join(ruta, \"stores\")\n",
    "views_path = os.path.join(ruta, \"views\")\n",
    "\n",
    "zip_path = os.path.join(stores_path, \"mlunlp-2024-ps-2.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hogares_path = \"train_hogares.csv\"\n",
    "train_personas_path = \"train_personas.csv\"\n",
    "test_hogares_path = \"test_hogares.csv\"\n",
    "test_personas_path = \"test_personas.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_hogares_path) as f:\n",
    "        train_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(train_personas_path) as f:\n",
    "        train_personas = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_hogares_path) as f:\n",
    "        test_hogares = pd.read_csv(f)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    with z.open(test_personas_path) as f:\n",
    "        test_personas = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = pd.merge(train_personas, train_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  \n",
    "test_merged = pd.merge(test_personas, test_hogares, on='id', how= \"inner\", suffixes=('_ind', '_hogar'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación y transformación de variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuestiones a tener en cuenta: \n",
    "\n",
    "Se cambió el formato de las variables \"P6210\", \"P6100\", \"P6585s3\", \"P6585s1\" de float a entero.\n",
    "Hacerlo también para las bases de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'Estrato1', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6500', 'P6510', 'P6510s1', 'P6510s2', 'P6545', 'P6545s1', 'P6545s2', 'P6580', 'P6580s1', 'P6580s2', 'P6585s1', 'P6585s1a1', 'P6585s1a2', 'P6585s2', 'P6585s2a1', 'P6585s2a2', 'P6585s3', 'P6585s3a1', 'P6585s3a2', 'P6585s4', 'P6585s4a1', 'P6585s4a2', 'P6590', 'P6590s1', 'P6600', 'P6600s1', 'P6610', 'P6610s1', 'P6620', 'P6620s1', 'P6630s1', 'P6630s1a1', 'P6630s2', 'P6630s2a1', 'P6630s3', 'P6630s3a1', 'P6630s4', 'P6630s4a1', 'P6630s6', 'P6630s6a1', 'P6750', 'P6760', 'P550', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7070', 'P7090', 'P7110', 'P7120', 'P7140s1', 'P7140s2', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7422s1', 'P7472', 'P7472s1', 'P7495', 'P7500s1', 'P7500s1a1', 'P7500s2', 'P7500s2a1', 'P7500s3', 'P7500s3a1', 'P7505', 'P7510s1', 'P7510s1a1', 'P7510s2', 'P7510s2a1', 'P7510s3', 'P7510s3a1', 'P7510s5', 'P7510s5a1', 'P7510s6', 'P7510s6a1', 'P7510s7', 'P7510s7a1', 'Pet', 'Oc', 'Des', 'Ina', 'Impa', 'Isa', 'Ie', 'Imdi', 'Iof1', 'Iof2', 'Iof3h', 'Iof3i', 'Iof6', 'Cclasnr2', 'Cclasnr3', 'Cclasnr4', 'Cclasnr5', 'Cclasnr6', 'Cclasnr7', 'Cclasnr8', 'Cclasnr11', 'Impaes', 'Isaes', 'Iees', 'Imdies', 'Iof1es', 'Iof2es', 'Iof3hes', 'Iof3ies', 'Iof6es', 'Ingtotob', 'Ingtotes', 'Ingtot', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Ingtotug', 'Ingtotugarr', 'Ingpcug', 'Li', 'Lp', 'Pobre', 'Indigente', 'Npobres', 'Nindigentes', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n",
      "['id', 'Orden', 'Clase_ind', 'Dominio_ind', 'P6020', 'P6040', 'P6050', 'P6090', 'P6100', 'P6210', 'P6210s1', 'P6240', 'Oficio', 'P6426', 'P6430', 'P6510', 'P6545', 'P6580', 'P6585s1', 'P6585s2', 'P6585s3', 'P6585s4', 'P6590', 'P6600', 'P6610', 'P6620', 'P6630s1', 'P6630s2', 'P6630s3', 'P6630s4', 'P6630s6', 'P6800', 'P6870', 'P6920', 'P7040', 'P7045', 'P7050', 'P7090', 'P7110', 'P7120', 'P7150', 'P7160', 'P7310', 'P7350', 'P7422', 'P7472', 'P7495', 'P7500s2', 'P7500s3', 'P7505', 'P7510s1', 'P7510s2', 'P7510s3', 'P7510s5', 'P7510s6', 'P7510s7', 'Pet', 'Oc', 'Des', 'Ina', 'Fex_c_ind', 'Depto_ind', 'Fex_dpto_ind', 'Clase_hogar', 'Dominio_hogar', 'P5000', 'P5010', 'P5090', 'P5100', 'P5130', 'P5140', 'Nper', 'Npersug', 'Li', 'Lp', 'Fex_c_hogar', 'Depto_hogar', 'Fex_dpto_hogar']\n"
     ]
    }
   ],
   "source": [
    "columnas = train_merged.columns.tolist()\n",
    "print(columnas)\n",
    "\n",
    "columnas2 = test_merged.columns.tolist()\n",
    "print(columnas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo la variable hacin\n",
    "train_merged[\"hacin\"] = train_merged[\"Nper\"] / train_merged[\"P5010\"]\n",
    "test_merged[\"hacin\"] = test_merged[\"Nper\"] / test_merged[\"P5010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 5 4 1 6 0 9]\n"
     ]
    }
   ],
   "source": [
    "# Transformo los valores de P6210\n",
    "train_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) \n",
    "print(train_merged[\"P6210\"].unique())\n",
    "\n",
    "test_merged[\"P6210\"] = train_merged[\"P6210\"].fillna(0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo el máximo nivel educativo por hogar\n",
    "educacion_max_hogar = train_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "educacion_max_hogar2 = test_merged.groupby(\"id\")[\"P6210\"].max().reset_index()\n",
    "\n",
    "# Renombro la columna\n",
    "educacion_max_hogar.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "educacion_max_hogar2.rename(columns={\"P6210\": \"educ_max\"}, inplace=True)\n",
    "\n",
    "train_merged = train_merged.merge(educacion_max_hogar, on=\"id\", how=\"left\")\n",
    "test_merged = test_merged.merge(educacion_max_hogar2, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6100\n",
    "train_merged[\"P6100\"] = train_merged[\"P6100\"].fillna(0).astype(int) \n",
    "test_merged[\"P6100\"] = test_merged[\"P6100\"].fillna(0).astype(int) \n",
    "\n",
    "# Creo dummy con personas de régimen de salud subsidiado\n",
    "test_merged[\"salud_subsi\"] = (test_merged[\"P6100\"] == 3).astype(int)\n",
    "train_merged[\"salud_subsi\"] = (train_merged[\"P6100\"] == 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s3\n",
    "train_merged[\"P6585s3\"] = train_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s3\"] = test_merged[\"P6585s3\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"fam_subsi\"] = (train_merged[\"P6585s3\"] == 1).astype(int)\n",
    "\n",
    "test_merged[\"fam_subsi\"] = (test_merged[\"P6585s3\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s1\n",
    "train_merged[\"P6585s1\"] = train_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s1\"] = test_merged[\"P6585s1\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"alim_subsi\"] = (train_merged[\"P6585s1\"] == 1).astype(int)\n",
    "test_merged[\"alim_subsi\"] = (test_merged[\"P6585s1\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo los valores de P6585s2\n",
    "train_merged[\"P6585s2\"] = train_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "test_merged[\"P6585s2\"] = test_merged[\"P6585s2\"].fillna(0).astype(int) \n",
    "\n",
    "# Asigno valor 1 a quienes reportan que sí y 0 en otro caso\n",
    "train_merged[\"transp_subsi\"] = (train_merged[\"P6585s2\"] == 1).astype(int)\n",
    "test_merged[\"transp_subsi\"] = (test_merged[\"P6585s2\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el primer approach\n",
    "train_pobreza = train_merged[[\"id\", \"Orden\", \"P6040\", \"hacin\", \"Clase_hogar\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"alim_subsi\", \"transp_subsi\", \"P6430\", \"Lp\", \"Pobre\"]]\n",
    "test_pobreza = test_merged[[\"id\", \"Orden\", \"P6040\", \"hacin\", \"Clase_hogar\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"alim_subsi\", \"transp_subsi\", \"P6430\", \"Lp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono las variables que podría llegar a utilizar para estimar pobreza con el segundo approach\n",
    "\n",
    "train_ingreso = train_merged[[\"id\", \"Orden\", \"Ingtot\", \"P6020\", \"P6040\", \"P6020\", \"P6920\", \"P6800\", \"P6210\", \"P6426\", \"P6430\",\"educ_max\", \"Lp\"]]\n",
    "test_ingreso = test_merged[[\"id\", \"Orden\", \"P6020\", \"P6040\", \"P6020\", \"P6920\", \"P6800\", \"P6210\", \"P6426\", \"P6430\",\"educ_max\", \"Lp\"]]\n",
    "\n",
    "# \"P6040\": edad, \n",
    "# \"P6020\": sexo, \n",
    "# \"P6920\": cotiza pension, \n",
    "# \"P6800\": horas de trabajo semanal normal, \n",
    "# \"P6210\": máximo nivel educativo, \n",
    "# \"P6426\": antigüedad, \n",
    "# \"P6920\": tipo de ocupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression\n",
      "Exactitud en entrenamiento: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90    116213\n",
      "           1       0.59      0.26      0.36     26524\n",
      "\n",
      "    accuracy                           0.83    142737\n",
      "   macro avg       0.72      0.61      0.63    142737\n",
      "weighted avg       0.80      0.83      0.80    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net\n",
      "Exactitud en entrenamiento: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90    116213\n",
      "           1       0.77      0.03      0.05     26524\n",
      "\n",
      "    accuracy                           0.82    142737\n",
      "   macro avg       0.79      0.51      0.47    142737\n",
      "weighted avg       0.81      0.82      0.74    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree\n",
      "Exactitud en entrenamiento: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    116213\n",
      "           1       1.00      0.96      0.98     26524\n",
      "\n",
      "    accuracy                           0.99    142737\n",
      "   macro avg       0.99      0.98      0.99    142737\n",
      "weighted avg       0.99      0.99      0.99    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree (max_depth=10)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.68      0.40      0.50     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.68      0.71    142737\n",
      "weighted avg       0.84      0.85      0.84    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest\n",
      "Exactitud en entrenamiento: 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    116213\n",
      "           1       0.99      0.97      0.98     26524\n",
      "\n",
      "    accuracy                           0.99    142737\n",
      "   macro avg       0.99      0.99      0.99    142737\n",
      "weighted avg       0.99      0.99      0.99    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Random Forest (n_estimators=200, max_depth=10)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91    116213\n",
      "           1       0.71      0.33      0.45     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.79      0.65      0.68    142737\n",
      "weighted avg       0.84      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.67      0.36      0.47     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.66      0.69    142737\n",
      "weighted avg       0.83      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (n_estimators=200, learning_rate=0.05)\n",
      "Exactitud en entrenamiento: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    116213\n",
      "           1       0.67      0.36      0.47     26524\n",
      "\n",
      "    accuracy                           0.85    142737\n",
      "   macro avg       0.77      0.66      0.69    142737\n",
      "weighted avg       0.83      0.85      0.83    142737\n",
      "\n",
      "----------------------------------------\n",
      "Resultados finales:\n",
      "Logistic Regression: 0.83\n",
      "Elastic Net: 0.82\n",
      "Decision Tree: 0.99\n",
      "Decision Tree (max_depth=10): 0.85\n",
      "Random Forest: 0.99\n",
      "Random Forest (n_estimators=200, max_depth=10): 0.85\n",
      "Gradient Boosting: 0.85\n",
      "Gradient Boosting (n_estimators=200, learning_rate=0.05): 0.85\n",
      "Proporción de hogares pobres predicha: 19.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Preprocesamiento de los datos\n",
    "# Selección de variables relevantes y eliminación de valores faltantes\n",
    "data_train = train_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\", \"Pobre\"]].dropna()\n",
    "data_test = test_pobreza[[\"id\", \"P6040\", \"hacin\", \"educ_max\", \"salud_subsi\", \"fam_subsi\", \"P6430\", \"Lp\"]].dropna()\n",
    "\n",
    "# Crear un dataset a nivel de hogar agrupando por id\n",
    "hogares_train = data_train.groupby('id').mean()\n",
    "hogares_train['Pobre'] = data_train.groupby('id')['Pobre'].max()  # Si algún miembro es pobre, se clasifica el hogar como pobre\n",
    "hogares_test = data_test.groupby('id').mean()\n",
    "\n",
    "# Definir variables predictoras y objetivo para entrenamiento\n",
    "X_train = hogares_train.drop(columns=['Pobre'])\n",
    "y_train = hogares_train['Pobre']\n",
    "\n",
    "# Definir variables predictoras para prueba\n",
    "X_test = hogares_test\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Decision Tree (max_depth=10)': DecisionTreeClassifier(max_depth=10),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest (n_estimators=200, max_depth=10)': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'Gradient Boosting (n_estimators=200, learning_rate=0.05)': GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "resultados = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_train)  # Evaluación en datos de entrenamiento\n",
    "    if nombre == 'Elastic Net':\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Convertir predicciones en binarias para Elastic Net\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    resultados[nombre] = accuracy\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"Exactitud en entrenamiento: {accuracy:.2f}\")\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"Resultados finales:\")\n",
    "for modelo, accuracy in resultados.items():\n",
    "    print(f\"{modelo}: {accuracy:.2f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_model_name = max(resultados, key=resultados.get)\n",
    "best_model = modelos[best_model_name]\n",
    "\n",
    "# Crear pipeline final\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Entrenar pipeline final\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones finales\n",
    "hogares_test['Predicted_Pobre'] = final_pipeline.predict(X_test)\n",
    "\n",
    "# Calcular proporción de hogares pobres en el conjunto de prueba\n",
    "prop_pobres = hogares_test['Predicted_Pobre'].mean()\n",
    "print(f\"Proporción de hogares pobres predicha: {prop_pobres:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': 0.8291262952142752,\n",
       " 'Elastic Net': 0.8175665734883036,\n",
       " 'Decision Tree': 0.9928189607459874,\n",
       " 'Decision Tree (max_depth=10)': 0.8526310627237507,\n",
       " 'Random Forest': 0.9927629136103463,\n",
       " 'Random Forest (n_estimators=200, max_depth=10)': 0.8506904306521785,\n",
       " 'Gradient Boosting': 0.8475798146240988,\n",
       " 'Gradient Boosting (n_estimators=200, learning_rate=0.05)': 0.847691908895381}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_12804\\22633242.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_ingreso['lp_total'] = test_ingreso.groupby('id')['Lp'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de los datos\n",
    "data_train = train_ingreso[[\"id\", \"P6020\", \"P6040\", \"P6800\", \"P6210\", \"Ingtot\"]].dropna()\n",
    "data_test = test_ingreso[[\"id\", \"P6020\", \"P6040\", \"P6800\", \"P6210\"]].dropna()\n",
    "\n",
    "# Genero la linea de pobreza de cada hogar\n",
    "test_ingreso['lp_total'] = test_ingreso.groupby('id')['Lp'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n",
    "# En un momento se duplicaron algunas columnas, corro lo siguiente para corregirlo si hiciera falta\n",
    "data_train = data_train.loc[:, ~data_train.columns.duplicated()]\n",
    "data_test = data_test.loc[:, ~data_test.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Linear Regression\n",
      "MSE en entrenamiento: 2136045751520.00\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net\n",
      "MSE en entrenamiento: 2137780816951.56\n",
      "----------------------------------------\n",
      "Modelo: Elastic Net (alpha=1)\n",
      "MSE en entrenamiento: 2170260168455.62\n",
      "----------------------------------------\n",
      "Modelo: Random Forest\n",
      "MSE en entrenamiento: 1697950602470.48\n",
      "----------------------------------------\n",
      "Modelo: Decision Tree (CART)\n",
      "MSE en entrenamiento: 1676482898085.78\n",
      "----------------------------------------\n",
      "Modelo: AdaBoost Regressor\n",
      "MSE en entrenamiento: 2246677490006.86\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (learning_rate=0.1)\n",
      "MSE en entrenamiento: 1909461105532.13\n",
      "----------------------------------------\n",
      "Modelo: Gradient Boosting (learning_rate=0.05)\n",
      "MSE en entrenamiento: 1925898229767.20\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asegurarse de que Ingtot esté en formato numérico\n",
    "data_train['Ingtot'] = pd.to_numeric(data_train['Ingtot'], errors='coerce')\n",
    "\n",
    "# Eliminar filas con valores faltantes en el conjunto de entrenamiento\n",
    "data_train = data_train.dropna()\n",
    "\n",
    "# Nombre de la columna que contiene el ID\n",
    "id_column = 'id'  # Cambia esto si el nombre de la columna ID es diferente\n",
    "\n",
    "# Excluir 'id' al definir las variables predictoras\n",
    "X = data_train.drop(columns=['Ingtot', id_column])\n",
    "y = data_train['Ingtot']\n",
    "\n",
    "X_train = X\n",
    "y_train = y\n",
    "\n",
    "# Modelos\n",
    "modelos = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Elastic Net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'Elastic Net (alpha=1)': ElasticNet(alpha=1, l1_ratio=0.7),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Decision Tree (CART)': DecisionTreeRegressor(random_state=42),\n",
    "    'AdaBoost Regressor': AdaBoostRegressor(random_state=42),\n",
    "    'Gradient Boosting (learning_rate=0.1)': GradientBoostingRegressor(learning_rate=0.1, random_state=42),\n",
    "    'Gradient Boosting (learning_rate=0.05)': GradientBoostingRegressor(learning_rate=0.05, random_state=42),\n",
    "}\n",
    "\n",
    "# Entrenamiento y predicciones\n",
    "resultados = {}\n",
    "mejor_modelo = None\n",
    "mejor_mse = float('inf')\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    # Crear pipeline (escalado + modelo)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Escalado de variables\n",
    "        ('model', modelo)\n",
    "    ])\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones en el conjunto de entrenamiento (para evaluación)\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    resultados[nombre] = mse_train\n",
    "    \n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"MSE en entrenamiento: {mse_train:.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Verificar si es el mejor modelo\n",
    "    if mse_train < mejor_mse:\n",
    "        mejor_mse = mse_train\n",
    "        mejor_modelo = pipeline\n",
    "\n",
    "# Asegurarse de que las columnas coincidan entre X_train y X_test\n",
    "X_test = data_test[X_train.columns]\n",
    "\n",
    "# Predicciones finales en el conjunto de prueba\n",
    "data_test['Ingtot_predicho'] = mejor_modelo.predict(X_test)\n",
    "\n",
    "# Crear un DataFrame con los resultados finales (ID + Predicción del mejor modelo)\n",
    "resultados_finales = data_test[[id_column, 'Ingtot_predicho']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor modelo: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model', DecisionTreeRegressor(random_state=42))]) con MSE: 1676482898085.78\n",
      "                              id  Ingtot_predicho\n",
      "0       3279230a4917cdf883df34cd    711910.577236\n",
      "2       3279230a4917cdf883df34cd    481500.000000\n",
      "4       01bd1f72445acc719d19bd25    854634.335938\n",
      "5       01bd1f72445acc719d19bd25    300000.000000\n",
      "7       25b93bd4d1a5750f091a1d87    570891.770833\n",
      "...                          ...              ...\n",
      "219629  458c384bc707d1e6f6ad309f    637844.225989\n",
      "219632  3bb709b746493f547efd3048    587324.561404\n",
      "219633  3bb709b746493f547efd3048         0.000000\n",
      "219637  1d72b0a4a42bb553a1fc7b9d    910687.500000\n",
      "219638  1d72b0a4a42bb553a1fc7b9d    652763.301471\n",
      "\n",
      "[99940 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el mejor modelo y los resultados\n",
    "print(f\"\\nMejor modelo: {mejor_modelo} con MSE: {mejor_mse:.2f}\")\n",
    "print(resultados_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_12804\\2379261951.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['lp_total'] = test_ingreso['lp_total']\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_12804\\2379261951.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['ingtot_total'] = resultados_finales.groupby('id')['Ingtot_predicho'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de hogares por debajo de la línea de pobreza: 19.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_12804\\2379261951.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  resultados_finales['por_bajo_lp'] = resultados_finales.apply(lambda row: 1 if row['ingtot_total'] < row['lp_total'] else 0, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar que proporcion de los hogares son pobres\n",
    "resultados_finales['lp_total'] = test_ingreso['lp_total']\n",
    "resultados_finales['ingtot_total'] = resultados_finales.groupby('id')['Ingtot_predicho'].transform('sum')  # Suma de Lp de todos los miembros del hogar\n",
    "resultados_finales['por_bajo_lp'] = resultados_finales.apply(lambda row: 1 if row['ingtot_total'] < row['lp_total'] else 0, axis=1)\n",
    "porcentaje_bajo_lp = resultados_finales['por_bajo_lp'].mean() * 100\n",
    "print(f\"Proporción de hogares por debajo de la línea de pobreza: {porcentaje_bajo_lp:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
